import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
from sklearn.metrics import accuracy_score,confusion_matrix,classification_report
import os
from sklearn.ensemble import IsolationForest
from sklearn.naive_bayes import BernoulliNB


df = pd.read_csv(r'dataset\data.csv')


df.head()


#data analysis


df.info()


df.describe()


df.isnull().sum()


Labels = ['crop']

for i in Labels:
    df[i] = LabelEncoder().fit_transform(df[i])
df


labels = ['ON','OFF']
labels


sns.set(style="darkgrid") 
plt.figure(figsize=(12, 6)) 
ax = sns.countplot(x=df['pump'], data=df, palette="Set3")
plt.title("Count Plot")  
plt.xlabel("Categories") 
plt.ylabel("Count") 

ax.set_xticklabels(labels)

for p in ax.patches:
    ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()),
                ha='center', va='center', fontsize=10, color='black', xytext=(0, 5),
                textcoords='offset points')

plt.show()  


x = df.drop(['pump'],axis = 1)
x.head()


precision = []
recall = []
fscore = []
accuracy = []


def performance_metrics(algorithm, predict, testY):
    testY = testY.astype('int')
    predict = predict.astype('int')
    p = precision_score(testY, predict,average='macro') * 100
    r = recall_score(testY, predict,average='macro') * 100
    f = f1_score(testY, predict,average='macro') * 100
    a = accuracy_score(testY,predict)*100 
    accuracy.append(a)
    precision.append(p)
    recall.append(r)
    fscore.append(f)
    print(algorithm+' Accuracy    : '+str(a))
    print(algorithm+' Precision   : '+str(p))
    print(algorithm+' Recall      : '+str(r))
    print(algorithm+' FSCORE      : '+str(f))
    report=classification_report(predict, testY,target_names=labels)
    print('\n',algorithm+" classification report\n",report)
    conf_matrix = confusion_matrix(testY, predict) 
    plt.figure(figsize =(5, 5)) 
    ax = sns.heatmap(conf_matrix, xticklabels = labels, yticklabels = labels, annot = True, cmap="Blues" ,fmt ="g");
    ax.set_ylim([0,len(labels)])
    plt.title(algorithm+" Confusion matrix") 
    plt.ylabel('True class') 
    plt.xlabel('Predicted class') 
    plt.show()


bnb_model_path = 'model/BernoulliNBClassifier.npy'
if os.path.exists(bnb_model_path):
    # Load the Bernoulli Naive Bayes Classifier model
    bnb_classifier = np.load(bnb_model_path, allow_pickle=True).item()
else:                       
    # Train and save the Bernoulli Naive Bayes Classifier model
    bnb_classifier = BernoulliNB()
    bnb_classifier.fit(x_train, y_train)
    np.save(bnb_model_path, bnb_classifier)

# Predict using the trained Bernoulli Naive Bayes Classifier model
y_pred_bnb = bnb_classifier.predict(x_test)

# Evaluate the Bernoulli Naive Bayes Classifier model
performance_metrics('BernoulliNBClassifier', y_pred_bnb, y_test)


from sklearn.linear_model import RidgeClassifier

ridge_model_path = 'model/RidgeClassifier.npy'
if os.path.exists(ridge_model_path):
    ridge_classifier = np.load(ridge_model_path, allow_pickle=True).item()
else:                       
    ridge_classifier = RidgeClassifier()
    ridge_classifier.fit(x_train, y_train)
    np.save(ridge_model_path, ridge_classifier)

y_pred_ridge = ridge_classifier.predict(x_test)

performance_metrics('RidgeClassifier', y_pred_ridge, y_test)



#showing all algorithms performance values
columns = ["Algorithm Name","Precison","Recall","FScore","Accuracy"]
values = []
algorithm_names = ["BernoulliNB Classifier", "Ridge Classifier"]
for i in range(len(algorithm_names)):
    values.append([algorithm_names[i],precision[i],recall[i],fscore[i],accuracy[i]])
    
temp = pd.DataFrame(values,columns=columns)
temp


Test_Labels = ['crop']

for i in Test_Labels:
    test[i] = LabelEncoder().fit_transform(test[i])
test


predict = ridge_classifier.predict(test)

for i, p in enumerate(predict):
    if p == 0:
        print(test.iloc[i]) 
        print("Model Predicted of Row {} Test Data is--->".format(i),labels[0])
    elif p == 1:
        print(test.iloc[i])  
        print("Model Predicted of Row {} Test Data is--->".format(i),labels[1])
        

x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.30, random_state = 42)
